{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedSAM Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/bowang-lab/MedSAM.git\n",
    "%pip install segment-anything\n",
    "%pip install boxsdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 13:51:38.243274: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-14 13:51:39.813848: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from segment_anything import sam_model_registry\n",
    "from skimage import transform\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from boxsdk import OAuth2\n",
    "from boxsdk import Client\n",
    "import monai\n",
    "# from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader & Box Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection\n",
    "\n",
    "oauth = OAuth2(\n",
    "  client_id='4ux6qwjvcbp58iau0kgar3t2ceewq47x',\n",
    "  client_secret='pyYMweGpw6Y7W5WBiWE7bmxymJlDjqoB',\n",
    "  access_token='YgviOaU1NZ03Wj7CNvin704DQiWQ9kBk',\n",
    ")\n",
    "\n",
    "client = Client(oauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access MRI data folder\n",
    "\n",
    "SHARED_LINK_URL = 'https://app.box.com/file/1433849452284?s=swtqk78ia9uyl2lookmxh26pas75hgrk'\n",
    "shared_item = client.get_shared_item(SHARED_LINK_URL)\n",
    "\n",
    "with open('FileFromBox.xlsx', 'wb') as open_file:\n",
    "    client.with_shared_link(SHARED_LINK_URL, None).file(shared_item.id).download_to(open_file)\n",
    "    open_file.close()\n",
    "\n",
    "mri_data = pd.read_excel('FileFromBox.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Fields for MRI Data\n",
    "\n",
    "mri_data['Total Images'] = mri_data['Number of Slices'] + mri_data['Number of Brightness Levels']\n",
    "mri_data['Start_Index'] = 0\n",
    "mri_data['Has MRI'] = ~mri_data['PNG filtered MRI'].isna()\n",
    "mri_data['Has Seg'] = ~mri_data['PNG segmentation'].isna()\n",
    "folders = []\n",
    "for i in range(len(mri_data)):\n",
    "\n",
    "    brightness_folders = []\n",
    "    if mri_data['Has MRI'][i]:\n",
    "        try:\n",
    "            png = client.get_shared_item(mri_data['PNG filtered MRI'][i])\n",
    "\n",
    "            items = client.folder(png.id).get_items()\n",
    "            for item in items:\n",
    "                brightness_folders.append(item.id)\n",
    "                # brightness_items = client.folder(item.id).get_items()\n",
    "                # imgs = []\n",
    "                # for img in brightness_items:\n",
    "                #     imgs.append(img.id)\n",
    "                \n",
    "                # brightness_folders.append((item.id, imgs))\n",
    "        except:\n",
    "            print(f\"{mri_data['MRI/Patient ID'][i]} has error; verify folder addresses\")\n",
    "        \n",
    "    folders.append(brightness_folders)\n",
    "\n",
    "mri_data['Brightness Folders'] = folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only patients with both MRI and Seg PNG files ready\n",
    "mri_data = mri_data[(mri_data['Has MRI']) & (mri_data['Has Seg'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "train_data, test_data = train_test_split(mri_data, test_size=0.25)\n",
    "train_data = train_data.reset_index().drop(columns = 'index')\n",
    "test_data = test_data.reset_index().drop(columns = 'index')\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data.loc[i, 'Start_Index'] = sum(train_data['Total Images'][:i])\n",
    "    \n",
    "for i in range(len(test_data)):\n",
    "    test_data.loc[i, 'Start_Index'] = sum(test_data['Total Images'][:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset using Box access\n",
    "\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, labels, train = True, transform=None, target_transform=None):\n",
    "        self.img_labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        len = sum(self.img_labels['Total Images'])\n",
    "        return int(len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if train:\n",
    "            path = 'train/'\n",
    "        else:\n",
    "            path = 'test/'\n",
    "        row = self.img_labels.loc[self.img_labels.Start_Index.where(self.img_labels.Start_Index >= idx).first_valid_index()]\n",
    "        patient = row['MRI/Patient ID']\n",
    "        bright_level = int((idx - row['Start_Index']) % len(row['Brightness Folders']))\n",
    "        bright_id = row['Brightness Folders'][bright_level]\n",
    "\n",
    "        brightness_items = client.folder(bright_id).get_items()\n",
    "        imgs = []\n",
    "        for img in brightness_items:\n",
    "            imgs.append(img.id)\n",
    "\n",
    "        img_idx = int((idx - row['Start_Index']) % row['Number of Slices'])\n",
    "        with open(f'{path}png_{idx}.png', 'wb') as open_pic:\n",
    "            client.file(imgs[img_idx]).download_to(open_pic)\n",
    "            open_pic.close()\n",
    "\n",
    "        image = torch.Tensor(np.array(Image.open(f'{path}png_{idx}.png'), dtype='int16'))\n",
    "        \n",
    "        seg_folder = client.get_shared_item(row['PNG segmentation']).get_items()\n",
    "        seg_ids = []\n",
    "        for item in seg_folder:\n",
    "            seg_ids.append(item.id)\n",
    "\n",
    "        with open(f'{path}seg_{idx}.png', 'wb') as open_pic:\n",
    "            client.file(seg_ids[img_idx]).download_to(open_pic)\n",
    "            open_pic.close()\n",
    "\n",
    "        label = torch.Tensor(np.array(Image.open(f'{path}seg_{idx}.png'), dtype='int16'))\n",
    "\n",
    "        value_to_class = {\n",
    "            9362: 0, # Background\n",
    "            18724: 1, # Water\n",
    "            28086: 2, # Skin\n",
    "            37449: 3, # Fat\n",
    "            46811: 4, # FGT?\n",
    "            56173: 5, # Tumor\n",
    "            65535: 6 # Clip\n",
    "        }\n",
    "\n",
    "        label_classes = torch.zeros_like(label, dtype=torch.long)  # Initialize with zeros\n",
    "        for value, class_idx in value_to_class.items():\n",
    "            label_classes[label == value] = class_idx\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label_classes, patient, bright_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataset = CancerDataset(labels=train_data)\n",
    "test_dataset = CancerDataset(labels=test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MedSAM Evaluation: No Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # %% environment and functions\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# join = os.path.join\n",
    "# import torch\n",
    "# from segment_anything import sam_model_registry\n",
    "# from skimage import io, transform\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# visualization functions\n",
    "# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
    "# change color to avoid red and green\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251/255, 252/255, 30/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "@torch.no_grad()\n",
    "def medsam_inference(medsam_model, img_embed, box_1024, H, W):\n",
    "    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n",
    "    if len(box_torch.shape) == 2:\n",
    "        box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "\n",
    "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=box_torch,\n",
    "        masks=None,\n",
    "    )\n",
    "    low_res_logits, _ = medsam_model.mask_decoder(\n",
    "        image_embeddings=img_embed, # (B, 256, 64, 64)\n",
    "        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "        multimask_output=True,\n",
    "        )\n",
    "\n",
    "    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "\n",
    "    low_res_pred = F.interpolate(\n",
    "        low_res_pred,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )  # (1, 1, gt.shape)\n",
    "    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n",
    "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
    "    return medsam_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [512, 464] at entry 0 and [512, 512] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11613/1395148324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_level\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [512, 464] at entry 0 and [512, 512] at entry 1"
     ]
    }
   ],
   "source": [
    "#%% load model and image\n",
    "MedSAM_CKPT_PATH = \"medsam_vit_b.pth\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model.image_encoder.patch_embed.proj = nn.Conv2d(3, 768, kernel_size = (8, 8), stride = (8, 8), padding = (0, 24))\n",
    "device = \"cuda:0\"\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()\n",
    "\n",
    "test_acc = []\n",
    "\n",
    "for x, y, patient, b_level in test_loader:\n",
    "    img = x\n",
    "    B, H, W = img.size()\n",
    "    img_3c = img.repeat(3, 1, 1, 1).view(B, 3, H, W).to(device)\n",
    "\n",
    "    box_np = np.array([[0,0, W, H]]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = medsam_model.image_encoder(img_3c) # (1, 256, 64, 64)\n",
    "    \n",
    "    medsam_seg = medsam_inference(medsam_model, image_embedding, box_np, H, W)\n",
    "\n",
    "    acc = (torch.tensor(medsam_seg) == y).float().mean()\n",
    "    test_acc.append((acc, patient, b_level))\n",
    "\n",
    "    if i == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].imshow(img, cmap=\"gray\")\n",
    "        show_box(box_np[0], ax[0])\n",
    "        ax[0].set_title(\"Input Image and Bounding Box\")\n",
    "        ax[1].imshow(img, cmap=\"gray\")\n",
    "        show_mask(medsam_seg, ax[1])\n",
    "        show_box(box_np[0], ax[1])\n",
    "        ax[1].set_title(f\"MedSAM Segmentation w/ Accuracy: {acc}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(np.array(Image.open(f'{path}png_{idx}.png'), dtype='int16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results\n",
    "base_results = pd.DataFrame(test_acc, columns = ['Accuracy', 'Patient', 'Brightness Level'])\n",
    "base_results.to_csv(\"MRI_MedSAM_Base.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MedSAM Fine-tuning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsam_model.train()\n",
    "\n",
    "losses = []\n",
    "seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction=\"mean\")\n",
    "ce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "img_mask_encdec_params = list(medsam_model.image_encoder.parameters()) + list(medsam_model.mask_decoder.parameters())\n",
    "optimizer = torch.optim.AdamW(img_mask_encdec_params, lr=.0001, weight_decay=.01)\n",
    "\n",
    "train_acc = []\n",
    "\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    for x, y, patient, b_level in train_loader:\n",
    "        img = x\n",
    "        B, H, W = img.size()\n",
    "        img_3c = img.repeat(3, 1, 1, 1).view(B, 3, H, W)#.to(device)\n",
    "\n",
    "        box_np = np.array([[0,0, W, H]])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding = medsam_model.image_encoder(img_3c) # (1, 256, 64, 64)\n",
    "        \n",
    "        medsam_seg = medsam_inference(medsam_model, image_embedding, box_np, H, W)\n",
    "\n",
    "        acc = (torch.tensor(medsam_seg) == y).float().mean()\n",
    "        test_acc.append((acc, patient, b_level))\n",
    "\n",
    "        if i == 0:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].imshow(img, cmap=\"gray\")\n",
    "            show_box(box_np[0], ax[0])\n",
    "            ax[0].set_title(\"Input Image and Bounding Box\")\n",
    "            ax[1].imshow(img, cmap=\"gray\")\n",
    "            show_mask(medsam_seg, ax[1])\n",
    "            show_box(box_np[0], ax[1])\n",
    "            ax[1].set_title(f\"MedSAM Segmentation w/ Accuracy: {acc}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsam_model.eval()\n",
    "\n",
    "test_acc = []\n",
    "\n",
    "for x, y, patient, b_level in test_loader:\n",
    "    img = x\n",
    "    B, H, W = img.size()\n",
    "    img_3c = img.repeat(3, 1, 1, 1).view(B, 3, H, W)#.to(device)\n",
    "\n",
    "    box_np = np.array([[0,0, W, H]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = medsam_model.image_encoder(img_3c) # (1, 256, 64, 64)\n",
    "    \n",
    "    medsam_seg = medsam_inference(medsam_model, image_embedding, box_np, H, W)\n",
    "\n",
    "    acc = (torch.tensor(medsam_seg) == y).float().mean()\n",
    "    test_acc.append((acc, patient, b_level))\n",
    "\n",
    "    if i == 0:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].imshow(img, cmap=\"gray\")\n",
    "        show_box(box_np[0], ax[0])\n",
    "        ax[0].set_title(\"Input Image and Bounding Box\")\n",
    "        ax[1].imshow(img, cmap=\"gray\")\n",
    "        show_mask(medsam_seg, ax[1])\n",
    "        show_box(box_np[0], ax[1])\n",
    "        ax[1].set_title(f\"MedSAM Segmentation w/ Accuracy: {acc}\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
